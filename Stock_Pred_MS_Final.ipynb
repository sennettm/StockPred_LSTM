{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "republican-canberra",
   "metadata": {},
   "source": [
    "Whatever folder this file is in, make sure you have a models folder and the data folder containing the csv stock files. You just have to tell it what stock you're interested in. A file is written out that contains two columns. In the first column the overall accuracy of the stock predictions and in the second column is the class prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "several-hamilton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Val_Loss: 0.037934400141239166 | CR: 0.24333333333333335\n",
      "Epoch: 0 | Class Corr: 0.6766666666666666\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 1 | Val_Loss: 0.6437296867370605 | CR: 0.09333333333333334\n",
      "Epoch: 1 | Class Corr: 0.5933333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 2 | Val_Loss: 0.12701845169067383 | CR: 0.10666666666666667\n",
      "Epoch: 2 | Class Corr: 0.67\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 3 | Val_Loss: 0.09960348904132843 | CR: 0.19333333333333333\n",
      "Epoch: 3 | Class Corr: 0.6566666666666666\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 4 | Val_Loss: 0.04882657900452614 | CR: 0.19\n",
      "Epoch: 4 | Class Corr: 0.6533333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 5 | Val_Loss: 0.04259958490729332 | CR: 0.13666666666666666\n",
      "Epoch: 5 | Class Corr: 0.6933333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 6 | Val_Loss: 0.039697811007499695 | CR: 0.14666666666666667\n",
      "Epoch: 6 | Class Corr: 0.6733333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 7 | Val_Loss: 0.04868621379137039 | CR: 0.23333333333333334\n",
      "Epoch: 7 | Class Corr: 0.7133333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 8 | Val_Loss: 0.0451948307454586 | CR: 0.23\n",
      "Epoch: 8 | Class Corr: 0.7366666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 9 | Val_Loss: 0.02861941047012806 | CR: 0.20666666666666667\n",
      "Epoch: 9 | Class Corr: 0.7266666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 10 | Val_Loss: 0.024364452809095383 | CR: 0.21\n",
      "Epoch: 10 | Class Corr: 0.7133333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 11 | Val_Loss: 0.018933551385998726 | CR: 0.23\n",
      "Epoch: 11 | Class Corr: 0.7466666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 12 | Val_Loss: 0.01635236106812954 | CR: 0.28\n",
      "Epoch: 12 | Class Corr: 0.7633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 13 | Val_Loss: 0.01299266330897808 | CR: 0.37333333333333335\n",
      "Epoch: 13 | Class Corr: 0.7733333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 14 | Val_Loss: 0.013908335007727146 | CR: 0.43\n",
      "Epoch: 14 | Class Corr: 0.7866666666666666\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 15 | Val_Loss: 0.013893655501306057 | CR: 0.4633333333333333\n",
      "Epoch: 15 | Class Corr: 0.7833333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 16 | Val_Loss: 0.015562008135020733 | CR: 0.30333333333333334\n",
      "Epoch: 16 | Class Corr: 0.7433333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 17 | Val_Loss: 0.011469108983874321 | CR: 0.4666666666666667\n",
      "Epoch: 17 | Class Corr: 0.81\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 18 | Val_Loss: 0.010472739115357399 | CR: 0.4633333333333333\n",
      "Epoch: 18 | Class Corr: 0.8066666666666666\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 19 | Val_Loss: 0.010173781774938107 | CR: 0.41333333333333333\n",
      "Epoch: 19 | Class Corr: 0.81\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 20 | Val_Loss: 0.00968343298882246 | CR: 0.44666666666666666\n",
      "Epoch: 20 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 21 | Val_Loss: 0.009594532661139965 | CR: 0.4533333333333333\n",
      "Epoch: 21 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 22 | Val_Loss: 0.009592616930603981 | CR: 0.43666666666666665\n",
      "Epoch: 22 | Class Corr: 0.8133333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 23 | Val_Loss: 0.009566305205225945 | CR: 0.44666666666666666\n",
      "Epoch: 23 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 24 | Val_Loss: 0.009512964636087418 | CR: 0.44666666666666666\n",
      "Epoch: 24 | Class Corr: 0.8166666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 25 | Val_Loss: 0.009558792226016521 | CR: 0.42333333333333334\n",
      "Epoch: 25 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 26 | Val_Loss: 0.009418641217052937 | CR: 0.4633333333333333\n",
      "Epoch: 26 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 27 | Val_Loss: 0.009304863400757313 | CR: 0.48\n",
      "Epoch: 27 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 28 | Val_Loss: 0.009311620146036148 | CR: 0.43666666666666665\n",
      "Epoch: 28 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 29 | Val_Loss: 0.00904467049986124 | CR: 0.4766666666666667\n",
      "Epoch: 29 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 30 | Val_Loss: 0.00888589583337307 | CR: 0.48333333333333334\n",
      "Epoch: 30 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 31 | Val_Loss: 0.008854852057993412 | CR: 0.45\n",
      "Epoch: 31 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 32 | Val_Loss: 0.00858557503670454 | CR: 0.5\n",
      "Epoch: 32 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 33 | Val_Loss: 0.008390245027840137 | CR: 0.5033333333333333\n",
      "Epoch: 33 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 34 | Val_Loss: 0.008298983797430992 | CR: 0.4633333333333333\n",
      "Epoch: 34 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 35 | Val_Loss: 0.008101818151772022 | CR: 0.5166666666666667\n",
      "Epoch: 35 | Class Corr: 0.8166666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 36 | Val_Loss: 0.007913150824606419 | CR: 0.5133333333333333\n",
      "Epoch: 36 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 37 | Val_Loss: 0.007967330515384674 | CR: 0.46\n",
      "Epoch: 37 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 38 | Val_Loss: 0.007605630438774824 | CR: 0.52\n",
      "Epoch: 38 | Class Corr: 0.82\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 39 | Val_Loss: 0.007393002510070801 | CR: 0.52\n",
      "Epoch: 39 | Class Corr: 0.8266666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 40 | Val_Loss: 0.007200467400252819 | CR: 0.5433333333333333\n",
      "Epoch: 40 | Class Corr: 0.83\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 41 | Val_Loss: 0.0071456534788012505 | CR: 0.5433333333333333\n",
      "Epoch: 41 | Class Corr: 0.8333333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 42 | Val_Loss: 0.006933210883289576 | CR: 0.5433333333333333\n",
      "Epoch: 42 | Class Corr: 0.8266666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 43 | Val_Loss: 0.006865955889225006 | CR: 0.5366666666666666\n",
      "Epoch: 43 | Class Corr: 0.8233333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 44 | Val_Loss: 0.006710076238960028 | CR: 0.55\n",
      "Epoch: 44 | Class Corr: 0.8333333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 45 | Val_Loss: 0.006712007336318493 | CR: 0.5766666666666667\n",
      "Epoch: 45 | Class Corr: 0.84\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 46 | Val_Loss: 0.006532902829349041 | CR: 0.56\n",
      "Epoch: 46 | Class Corr: 0.84\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 47 | Val_Loss: 0.006422792095690966 | CR: 0.5533333333333333\n",
      "Epoch: 47 | Class Corr: 0.8366666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 48 | Val_Loss: 0.006307164207100868 | CR: 0.5666666666666667\n",
      "Epoch: 48 | Class Corr: 0.8333333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 49 | Val_Loss: 0.006346424110233784 | CR: 0.5833333333333334\n",
      "Epoch: 49 | Class Corr: 0.84\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 50 | Val_Loss: 0.006098219659179449 | CR: 0.5933333333333334\n",
      "Epoch: 50 | Class Corr: 0.8433333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 51 | Val_Loss: 0.006082449108362198 | CR: 0.56\n",
      "Epoch: 51 | Class Corr: 0.83\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 52 | Val_Loss: 0.006082537584006786 | CR: 0.59\n",
      "Epoch: 52 | Class Corr: 0.8466666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 53 | Val_Loss: 0.005904055666178465 | CR: 0.6066666666666667\n",
      "Epoch: 53 | Class Corr: 0.85\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 54 | Val_Loss: 0.005697733722627163 | CR: 0.6033333333333334\n",
      "Epoch: 54 | Class Corr: 0.8466666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 55 | Val_Loss: 0.005572404246777296 | CR: 0.6033333333333334\n",
      "Epoch: 55 | Class Corr: 0.8466666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 56 | Val_Loss: 0.005542438942939043 | CR: 0.6\n",
      "Epoch: 56 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 57 | Val_Loss: 0.005437984596937895 | CR: 0.6066666666666667\n",
      "Epoch: 57 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 58 | Val_Loss: 0.005276298616081476 | CR: 0.61\n",
      "Epoch: 58 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 59 | Val_Loss: 0.005142822861671448 | CR: 0.61\n",
      "Epoch: 59 | Class Corr: 0.85\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 60 | Val_Loss: 0.005166105926036835 | CR: 0.6366666666666667\n",
      "Epoch: 60 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 61 | Val_Loss: 0.005083742085844278 | CR: 0.63\n",
      "Epoch: 61 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 62 | Val_Loss: 0.005008781794458628 | CR: 0.6233333333333333\n",
      "Epoch: 62 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 63 | Val_Loss: 0.0049519301392138 | CR: 0.6266666666666667\n",
      "Epoch: 63 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 64 | Val_Loss: 0.0049471440725028515 | CR: 0.6366666666666667\n",
      "Epoch: 64 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 65 | Val_Loss: 0.004966306034475565 | CR: 0.65\n",
      "Epoch: 65 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 66 | Val_Loss: 0.004811848513782024 | CR: 0.6333333333333333\n",
      "Epoch: 66 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 67 | Val_Loss: 0.0047967140562832355 | CR: 0.63\n",
      "Epoch: 67 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 68 | Val_Loss: 0.0047318958677351475 | CR: 0.6433333333333333\n",
      "Epoch: 68 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 69 | Val_Loss: 0.004779416136443615 | CR: 0.66\n",
      "Epoch: 69 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 70 | Val_Loss: 0.004634391982108355 | CR: 0.6466666666666666\n",
      "Epoch: 70 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 71 | Val_Loss: 0.004590742755681276 | CR: 0.64\n",
      "Epoch: 71 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 72 | Val_Loss: 0.0045257932506501675 | CR: 0.6533333333333333\n",
      "Epoch: 72 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 73 | Val_Loss: 0.0046187834814190865 | CR: 0.6866666666666666\n",
      "Epoch: 73 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 74 | Val_Loss: 0.004522352013736963 | CR: 0.6933333333333334\n",
      "Epoch: 74 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 75 | Val_Loss: 0.00442979671061039 | CR: 0.65\n",
      "Epoch: 75 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 76 | Val_Loss: 0.004380517639219761 | CR: 0.6566666666666666\n",
      "Epoch: 76 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 77 | Val_Loss: 0.004440648481249809 | CR: 0.6833333333333333\n",
      "Epoch: 77 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 78 | Val_Loss: 0.0043909125961363316 | CR: 0.6833333333333333\n",
      "Epoch: 78 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 79 | Val_Loss: 0.004264099523425102 | CR: 0.6466666666666666\n",
      "Epoch: 79 | Class Corr: 0.8566666666666667\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "\n",
      "Epoch: 80 | Val_Loss: 0.00425325334072113 | CR: 0.66\n",
      "Epoch: 80 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 81 | Val_Loss: 0.0042058187536895275 | CR: 0.6766666666666666\n",
      "Epoch: 81 | Class Corr: 0.8533333333333334\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 82 | Val_Loss: 0.0042961593717336655 | CR: 0.6833333333333333\n",
      "Epoch: 82 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 83 | Val_Loss: 0.004258580971509218 | CR: 0.6833333333333333\n",
      "Epoch: 83 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 84 | Val_Loss: 0.0041662477888166904 | CR: 0.6833333333333333\n",
      "Epoch: 84 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 85 | Val_Loss: 0.0041434685699641705 | CR: 0.6766666666666666\n",
      "Epoch: 85 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 86 | Val_Loss: 0.004127693362534046 | CR: 0.6733333333333333\n",
      "Epoch: 86 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 87 | Val_Loss: 0.004119297489523888 | CR: 0.6933333333333334\n",
      "Epoch: 87 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 88 | Val_Loss: 0.0041427225805819035 | CR: 0.7033333333333334\n",
      "Epoch: 88 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 89 | Val_Loss: 0.004095160868018866 | CR: 0.69\n",
      "Epoch: 89 | Class Corr: 0.8566666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 90 | Val_Loss: 0.004044025670737028 | CR: 0.6933333333333334\n",
      "Epoch: 90 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 91 | Val_Loss: 0.0040231808088719845 | CR: 0.69\n",
      "Epoch: 91 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 92 | Val_Loss: 0.004003903362900019 | CR: 0.6833333333333333\n",
      "Epoch: 92 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 93 | Val_Loss: 0.003989407327026129 | CR: 0.7033333333333334\n",
      "Epoch: 93 | Class Corr: 0.86\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 94 | Val_Loss: 0.003989906050264835 | CR: 0.71\n",
      "Epoch: 94 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 95 | Val_Loss: 0.003967118449509144 | CR: 0.7133333333333334\n",
      "Epoch: 95 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 96 | Val_Loss: 0.003930466249585152 | CR: 0.7033333333333334\n",
      "Epoch: 96 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 97 | Val_Loss: 0.003904064418748021 | CR: 0.69\n",
      "Epoch: 97 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 98 | Val_Loss: 0.003888207720592618 | CR: 0.7033333333333334\n",
      "Epoch: 98 | Class Corr: 0.8666666666666667\n",
      "\n",
      "Model saved\n",
      "\n",
      "Epoch: 99 | Val_Loss: 0.003890631953254342 | CR: 0.7066666666666667\n",
      "Epoch: 99 | Class Corr: 0.8633333333333333\n",
      "\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "##########################################################\n",
    "STOCK = \"VZ\"\n",
    "##########################################################\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 20      # rnn input size\n",
    "output_size = 5\n",
    "lr = 0.02\n",
    "batch_size = 12\n",
    "num_epochs = 100\n",
    "seq_length = 5\n",
    "threshold = .02\n",
    "\n",
    "##########################################################\n",
    "#no changes here\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,     # rnn hidden unit\n",
    "            num_layers=1,       # number of rnn layer\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.out = nn.Linear(128, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = []    # save all predictions\n",
    "        for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        out = torch.stack(outs, dim=1)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out, h_state\n",
    "\n",
    "\n",
    "def train(stock):\n",
    "    ##########################################################\n",
    "    #created a file to store test accuracy\n",
    "    file1 = open(\"ClassAccur_\"+str(stock)+\".txt\",\"a\")\n",
    "    csv_file=\"./data/\"+str(stock)+\".csv\"\n",
    "    df = pd.read_csv(csv_file, parse_dates=['Date']).sort_values(by='Date')\n",
    "\n",
    "    cp = df['Close'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    cp = cp[-1000:, :]\n",
    "    scaler = MinMaxScaler().fit(cp)\n",
    "    norm_scale = scaler.transform(cp)\n",
    "    df = pd.DataFrame(norm_scale, columns=['Close'])\n",
    "    \n",
    "    ##########################################################\n",
    "    #wanted to automate the number of days to use for prediction\n",
    "    prev = []\n",
    "\n",
    "    for i in range(input_size):\n",
    "        col = 'prev'+str(i+1)\n",
    "        prev.append(col)\n",
    "\n",
    "    for i in range(len(prev)):\n",
    "        df[prev[i]] = df['Close'].shift(i+1)\n",
    "\n",
    "\n",
    "    df = df.dropna(subset=[prev[-1]]).reset_index(drop=True)\n",
    "\n",
    "    ##########################################################\n",
    "    #x is the prediction day input\n",
    "    #y is the 1 day output\n",
    "    x = df[prev].to_numpy(dtype=np.float32)\n",
    "    y = df['Close'].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    #create new arrays, z & w\n",
    "    #z is the 5 day output instead of 1 day\n",
    "    #w is the predefined shifted input days\n",
    "    \n",
    "    z=[]\n",
    "    for i in range(0, len(y)-4, 20):\n",
    "        z.append(y[i:i+output_size])\n",
    "    w=[]\n",
    "    for i in range(0, len(x)-4, 20):\n",
    "        w.append(x[i])\n",
    "    \n",
    "    w=np.array(w).astype(dtype=np.float32)\n",
    "    w=np.reshape(w, (-1, input_size))   \n",
    "    z=np.array(z).astype(dtype=np.float32)\n",
    "    z=np.reshape(z, (-1, output_size))\n",
    "\n",
    "    ###########################################################\n",
    "    #creating the dataset to be passed into dataloaders\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(0, len(w) - seq_length + 1, 1):\n",
    "        xs.append(w[i : i + seq_length, :].copy())\n",
    "        ys.append(z[i : i + seq_length, :].copy())\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.25)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,      # torch TensorDataset format\n",
    "        batch_size=batch_size,      # mini batch size\n",
    "        shuffle=True               # random shuffle for training\n",
    "    )\n",
    "\n",
    "\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,      # torch TensorDataset format\n",
    "        batch_size=batch_size,      # mini batch size\n",
    "        shuffle=True               # random shuffle for training\n",
    "    )\n",
    "\n",
    "    rnn = RNN().to(device)\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)   # optimize all cnn parameters\n",
    "    loss_func = nn.MSELoss()\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.view(-1, seq_length, input_size).type(torch.float).to(device)            \n",
    "            \n",
    "            labels = labels.view(-1, seq_length * output_size).type(torch.float).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = rnn(images, h_state=None)\n",
    "            loss = loss_func(outputs.view(-1, seq_length * output_size), labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() #retain_graph=True\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 1 == 0 and i == 0:\n",
    "                tot_tru_cls = []\n",
    "                tot_prd_cls = []\n",
    "                for j, (t_x, t_y) in enumerate(test_loader):\n",
    "                    ##########################################################\n",
    "                    #get the batch_size, seq_length, and output size for reshaping\n",
    "                    #arrays later\n",
    "                    bt_sz = t_y.size(0)\n",
    "                    seq_len = t_y.size(1)\n",
    "                    pred_wd = t_y.size(2)\n",
    "                    #take and convert the normalized last price from input set\n",
    "                    #into the actual stock value (to do inverse_transform, must be 1d array)\n",
    "                    last_pr = scaler.inverse_transform(t_x[:, :, 0].reshape(-1, 1))\n",
    "                    last_pr = np.reshape(last_pr, (bt_sz, seq_len, 1))\n",
    "                    #create an empty true class & empty pred class\n",
    "                    #index 0:4 the class order will be 5% dec, 2% dec, 0%, 2% inc, 5% inc\n",
    "                    tru_cls = np.zeros((bt_sz, seq_len, pred_wd))\n",
    "                    prd_cls = np.zeros((bt_sz, seq_len, pred_wd))\n",
    "                    #create the absolute bounds for class\n",
    "                    last_pr_02 = last_pr * 0.02\n",
    "                    last_pr_05 = last_pr * 0.05\n",
    "                    #take and convert the normalized true prices\n",
    "                    #into the actual stock value, reshape \n",
    "                    true_pr = scaler.inverse_transform(t_y.reshape(-1,1))\n",
    "                    true_pr = np.reshape(true_pr, (bt_sz, seq_len, pred_wd))\n",
    "                    \n",
    "                    ##########################################################\n",
    "                    #go through each true price and check if it decreased by at least\n",
    "                    #5%, decreased by at least 2%, increased by at least 5%, increased\n",
    "                    #by at least 2%, or doesn't change by 2%\n",
    "                    #only count class once\n",
    "                    for m in range(len(true_pr)):\n",
    "                        for n in range(len(true_pr[m])):\n",
    "                            for o in range(len(true_pr[m][n])):\n",
    "                                if true_pr[m][n][o] <= last_pr[m][n]-last_pr_05[m][n] and tru_cls[m][n][0] < 1:\n",
    "                                    tru_cls[m][n][0] += 1\n",
    "                                    tru_cls[m][n][1] += 1\n",
    "                                if true_pr[m][n][o] <= last_pr[m][n]-last_pr_02[m][n] and tru_cls[m][n][1] < 1:\n",
    "                                    tru_cls[m][n][1] += 1\n",
    "                                if true_pr[m][n][o] >= last_pr[m][n]+last_pr_05[m][n] and tru_cls[m][n][4] < 1:\n",
    "                                    tru_cls[m][n][4] += 1\n",
    "                                    tru_cls[m][n][3] += 1\n",
    "                                if true_pr[m][n][o] >= last_pr[m][n]+last_pr_02[m][n] and tru_cls[m][n][3] < 1:\n",
    "                                    tru_cls[m][n][3] += 1\n",
    "                                if tru_cls[m][n][2] < 1:\n",
    "                                    tru_cls[m][n][2] += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                                    \n",
    "\n",
    "                    t_x = t_x.view(-1, seq_length, input_size).type(torch.float).to(device)                 \n",
    "                    t_y = t_y.view(-1, seq_length * output_size).type(torch.float).to(device)  \n",
    "                    test_output, _ = rnn(t_x, h_state=None)\n",
    "\n",
    "                    ##########################################################\n",
    "                    #take and convert the normalized predicted prices\n",
    "                    #into the actual stock value, reshape \n",
    "                    pred_pr = scaler.inverse_transform(test_output.cpu().data.numpy().reshape(-1, 1))\n",
    "                    pred_pr = np.reshape(pred_pr, (bt_sz, seq_len, pred_wd))\n",
    "                    \n",
    "                    #go through each predicted price and check if it decreased by at least\n",
    "                    #5%, decreased by at least 2%, increased by at least 5%, increased\n",
    "                    #by at least 2%, or doesn't change by 2%\n",
    "                    #only count class once\n",
    "                    for m in range(len(true_pr)):\n",
    "                        for n in range(len(true_pr[m])):\n",
    "                            for o in range(len(true_pr[m][n])):\n",
    "                                if pred_pr[m][n][o] <= last_pr[m][n]-last_pr_05[m][n] and prd_cls[m][n][0] < 1:\n",
    "                                    prd_cls[m][n][0] += 1\n",
    "                                    prd_cls[m][n][1] += 1\n",
    "                                if pred_pr[m][n][o] <= last_pr[m][n]-last_pr_02[m][n] and prd_cls[m][n][1] < 1:\n",
    "                                    prd_cls[m][n][1] += 1\n",
    "                                if pred_pr[m][n][o] >= last_pr[m][n]+last_pr_05[m][n] and prd_cls[m][n][4] < 1:\n",
    "                                    prd_cls[m][n][4] += 1\n",
    "                                    prd_cls[m][n][3] += 1\n",
    "                                if pred_pr[m][n][o] >= last_pr[m][n]+last_pr_02[m][n] and prd_cls[m][n][3] < 1:\n",
    "                                    prd_cls[m][n][3] += 1\n",
    "                                if prd_cls[m][n][2] < 1:\n",
    "                                    prd_cls[m][n][2] += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                    tru_cls = np.where(tru_cls > 1, 1, tru_cls)\n",
    "                    prd_cls = np.where(prd_cls > 1, 1, prd_cls)\n",
    "                    test_loss = loss_func(test_output.view(-1, seq_length * output_size), t_y)                    \n",
    "                    t_out = test_output.cpu().data.numpy().reshape(-1, 1)                    \n",
    "                    t_y = t_y.cpu().data.numpy().reshape(-1, 1)                    \n",
    "                    t_out = scaler.inverse_transform(t_out)                    \n",
    "                    t_y = scaler.inverse_transform(t_y)\n",
    "                    \n",
    "\n",
    "                    #define correct as the prediction being off less than 2%\n",
    "                    correct = np.count_nonzero(np.absolute(t_out - t_y) <= threshold*t_y) / len(t_y)\n",
    "                    \n",
    "                    ##########################################################\n",
    "                    #this compares the prediction class to the true class\n",
    "                    #and sums up all correct class predictions and divides by \n",
    "                    #the total number of possible predictions\n",
    "\n",
    "                    total_correct = (prd_cls == tru_cls).sum().item()\n",
    "                    total_num = len(prd_cls)*len(prd_cls[0])*len(prd_cls[0][0])\n",
    "                    cls_acc = total_correct/total_num\n",
    "                    \n",
    "                    if j == 0:\n",
    "                        print('Epoch: ' + str(epoch) + ' | Val_Loss: ' + str(test_loss.item()) + ' | CR: ' + str(correct))\n",
    "                        print('Epoch: '+ str(epoch)+ ' | Class Corr: '+ str(cls_acc)+'\\n')\n",
    "                        file1.write(str(correct)+'\\t'+str(cls_acc)+'\\n')\n",
    "                    tot_tru_cls.append(tru_cls.sum(axis=0).sum(axis=0))\n",
    "                    tot_prd_cls.append(prd_cls.sum(axis=0).sum(axis=0))\n",
    "                tot_tru_cls=np.asarray(tot_tru_cls)\n",
    "                tot_prd_cls=np.asarray(tot_prd_cls)\n",
    "\n",
    "        # decay learning rate\n",
    "        if((epoch + 1) % 20) == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "                \n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(rnn.state_dict(), './models/model_' + str(epoch) + '.pkl')\n",
    "            print('Model saved\\n')\n",
    "    file1.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(stock = STOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-report",
   "metadata": {},
   "source": [
    "The following is what I used to generate predictions. I just copied the RNN model from above. I am analyzing a stock called VZ.csv. VZ.csv is just the GPS stock from October to April 28th. This returns prd_cls[-1][-1] whichs is the very last prediction class for based on April 28th. The five classes are decrease by 5%, decrease by 2%, no change, increase by 2%, and increase by 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disabled-comedy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VZ\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "##########################################################\n",
    "STOCK = \"VZ\"\n",
    "##########################################################\n",
    "input_size = 20\n",
    "output_size = 5\n",
    "lr = 0.02\n",
    "batch_size = 12\n",
    "num_epochs = 10\n",
    "seq_length = 5\n",
    "threshold = .02\n",
    "\n",
    "##########################################################\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(128, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        out = torch.stack(outs, dim=1)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out, h_state\n",
    "\n",
    "csv_file=\"./data/\"+str(STOCK)+\".csv\"\n",
    "df = pd.read_csv(csv_file, parse_dates=['Date']).sort_values(by='Date')\n",
    "cp = df['Close'].to_numpy().reshape(-1, 1)\n",
    "cp = cp[-150:]\n",
    "\n",
    "scaler = MinMaxScaler().fit(cp)\n",
    "norm_scale = scaler.transform(cp)\n",
    "df = pd.DataFrame(norm_scale, columns=['Close'])\n",
    "\n",
    "##########################################################\n",
    "#grab 25 days for prediction\n",
    "prev = []\n",
    "for i in range(input_size):\n",
    "    col = 'prev'+str(i)\n",
    "    prev.append(col)\n",
    "for i in range(len(prev)):\n",
    "    df[prev[i]] = df['Close'].shift(i)\n",
    "df = df.dropna(subset=[prev[-1]]).reset_index(drop=True)\n",
    "\n",
    "##########################################################\n",
    "#x is the prediction day input\n",
    "x = df[prev].to_numpy(dtype=np.float32)\n",
    "    \n",
    "w=[]\n",
    "for i in range(0, len(x)):\n",
    "    w.append(x[i])\n",
    "\n",
    "    \n",
    "w=np.array(w).astype(dtype=np.float32)\n",
    "w=np.reshape(w, (-1, input_size))  \n",
    "\n",
    "xs=[]    \n",
    "for i in range(0, len(w) - seq_length + 1, 1):\n",
    "    xs.append(w[i : i + seq_length, :].copy())\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#I needed to create a yt for some reason in order to get dataloader to \n",
    "#output the proper xt size for the prediction\n",
    "xt = np.array(xs)\n",
    "yt = xt\n",
    "\n",
    "xt = torch.from_numpy(xt)\n",
    "yt = torch.from_numpy(yt)\n",
    "\n",
    "\n",
    "test_data = Data.TensorDataset(xt, yt)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=12, shuffle=False)\n",
    "\n",
    "\n",
    "rnn = RNN().to(device)\n",
    "rnn.load_state_dict(torch.load('models/model_98.pkl'))\n",
    "\n",
    "def test(model, test_loader, device, stock):\n",
    "    print(stock)\n",
    "    file1 = open(\"ClassPred_\"+str(stock)+\".txt\",\"a\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t_x = test_loader\n",
    "\n",
    "        bt_sz = t_x.size(0)\n",
    "        seq_len = t_x.size(1)\n",
    "        pred_wd = output_size\n",
    "\n",
    "        last_pr = scaler.inverse_transform(t_x[:, :, 0].reshape(-1, 1))\n",
    "        last_pr = np.reshape(last_pr, (bt_sz, seq_len, 1))\n",
    "        last_pr_02 = last_pr * 0.02\n",
    "        last_pr_05 = last_pr * 0.05\n",
    "        prd_cls = np.zeros((bt_sz, seq_len, pred_wd))\n",
    "\n",
    "        test_output, _ = rnn(t_x, h_state=None)\n",
    "        pred_pr = scaler.inverse_transform(test_output.cpu().data.numpy().reshape(-1, 1))\n",
    "        pred_pr = np.reshape(pred_pr, (bt_sz, seq_len, pred_wd))\n",
    "        for m in range(len(prd_cls)):\n",
    "            for n in range(len(prd_cls[m])):\n",
    "                for o in range(len(prd_cls[m][n])):\n",
    "                    if pred_pr[m][n][o] <= last_pr[m][n]-last_pr_05[m][n] and prd_cls[m][n][0] < 1:\n",
    "                        prd_cls[m][n][0] += 1\n",
    "                        prd_cls[m][n][1] += 1\n",
    "                    if pred_pr[m][n][o] <= last_pr[m][n]-last_pr_02[m][n] and prd_cls[m][n][1] < 1:\n",
    "                        prd_cls[m][n][1] += 1\n",
    "                    if pred_pr[m][n][o] >= last_pr[m][n]+last_pr_05[m][n] and prd_cls[m][n][4] < 1:\n",
    "                        prd_cls[m][n][4] += 1\n",
    "                        prd_cls[m][n][3] += 1\n",
    "                    if pred_pr[m][n][o] >= last_pr[m][n]+last_pr_02[m][n] and prd_cls[m][n][3] < 1:\n",
    "                        prd_cls[m][n][3] += 1\n",
    "                    if prd_cls[m][n][2] < 1:\n",
    "                        prd_cls[m][n][2] += 1\n",
    "                    else:\n",
    "                        pass \n",
    "\n",
    "    prd_cls = np.where(prd_cls > 1, 1, prd_cls)\n",
    "    last_prd = prd_cls[-1][-1]\n",
    "    print(last_prd)\n",
    "    \n",
    "    file1.write(\"VZ\"+'\\t'+str(last_prd))\n",
    "    file1.close()\n",
    "    return prd_cls[-1][-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test(rnn, xt, device, STOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-blink",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
